
@misc{mediapipe_hands,
  doi = {10.48550/ARXIV.2006.10214},
  url = {https://arxiv.org/abs/2006.10214}, 
  author = {Zhang, Fan and Bazarevsky, Valentin and Vakunov, Andrey and Tkachenka, Andrei and Sung, George and Chang, Chuo-Ling and Grundmann, Matthias},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {MediaPipe Hands: On-device Real-time Hand Tracking},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@Article{drones_hololens,
AUTHOR = {Konstantoudakis, Konstantinos and Christaki, Kyriaki and Tsiakmakis, Dimitrios and Sainidis, Dimitrios and Albanis, Georgios and Dimou, Anastasios and Daras, Petros},
TITLE = {Drone Control in AR: An Intuitive System for Single-Handed Gesture Control, Drone Tracking, and Contextualized Camera Feed Visualization in Augmented Reality},
JOURNAL = {Drones},
VOLUME = {6},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {43},
URL = {https://www.mdpi.com/2504-446X/6/2/43},
ISSN = {2504-446X},
ABSTRACT = {Traditional drone handheld remote controllers, although well-established and widely used, are not a particularly intuitive control method. At the same time, drone pilots normally watch the drone video feed on a smartphone or another small screen attached to the remote. This forces them to constantly shift their visual focus from the drone to the screen and vice-versa. This can be an eye-and-mind-tiring and stressful experience, as the eyes constantly change focus and the mind struggles to merge two different points of view. This paper presents a solution based on Microsoft&rsquo;s HoloLens 2 headset that leverages augmented reality and gesture recognition to make drone piloting easier, more comfortable, and more intuitive. It describes a system for single-handed gesture control that can achieve all maneuvers possible with a traditional remote, including complex motions; a method for tracking a real drone in AR to improve flying beyond line of sight or at distances where the physical drone is hard to see; and the option to display the drone&rsquo;s live video feed in AR, either in first-person-view mode or in context with the environment.},
DOI = {10.3390/drones6020043}
}

@INPROCEEDINGS{jones_skin_color,
  author={Jones, M.J. and Rehg, J.M.},
  booktitle={Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149)}, 
  title={Statistical color models with application to skin detection}, 
  year={1999},
  volume={1},
  number={},
  pages={274-280 Vol. 1},
  doi={10.1109/CVPR.1999.786951}}


@ARTICLE{lee_hand_ar,
  author={Lee, Taehee and Hollerer, Tobias},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Multithreaded Hybrid Feature Tracking for Markerless Augmented Reality}, 
  year={2009},
  volume={15},
  number={3},
  pages={355-368},
  doi={10.1109/TVCG.2008.190}}


@Article{kim_leap_ar,
author={Kim, Minseok
and Lee, Jae Yeol},
title={Touch and hand gesture-based interactions for directly manipulating 3D virtual objects in mobile augmented reality},
journal={Multimedia Tools and Applications},
year={2016},
month={Dec},
day={01},
volume={75},
number={23},
pages={16529-16550},
issn={1573-7721},
doi={10.1007/s11042-016-3355-9},
url={https://doi.org/10.1007/s11042-016-3355-9}
}



@ARTICLE{canterbury_hand_ar,
  author={Billinghurst, Mark and Piumsomboon, Tham and Bai, Huidong},
  journal={IEEE Computer Graphics and Applications}, 
  title={Hands in Space: Gesture Interaction with Augmented-Reality Interfaces}, 
  year={2014},
  volume={34},
  number={1},
  pages={77-80},
  doi={10.1109/MCG.2014.8}}


@article{hand_ar_shen,
Author = {Shen, Y. and Ong, S. K. and Nee, A. Y. C.},
ISSN = {10447318},
Journal = {International Journal of Human-Computer Interaction},
Keywords = {HUMAN-computer interaction, VIRTUAL reality, HAND exercises, COMPUTER interfaces, GESTURE},
Number = {6},
Pages = {523 - 544},
Title = {Vision-Based Hand Interaction in Augmented Reality Environment.},
Volume = {27},
URL = {https://search-ebscohost-com.uplib.idm.oclc.org/login.aspx?direct=true&db=a9h&AN=60540089&site=ehost-live&scope=site},
Year = {2011},
}

@InProceedings{Du2Net,
author="Zhang, Yinda
and Wadhwa, Neal
and Orts-Escolano, Sergio
and H{\"a}ne, Christian
and Fanello, Sean
and Garg, Rahul",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="Du2Net: Learning Depth Estimation from Dual-Cameras and Dual-Pixels",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="582--598",
abstract="Computational stereo has reached a high level of accuracy, but degrades in the presence of occlusions, repeated textures, and correspondence errors along edges. We present a novel approach based on neural networks for depth estimation that combines stereo from dual cameras with stereo from a dual-pixel sensor, which is increasingly common on consumer cameras. Our network uses a novel architecture to fuse these two sources of information and can overcome the above-mentioned limitations of pure binocular stereo matching. Our method provides a dense depth map with sharp edges, which is crucial for computational photography applications like synthetic shallow-depth-of-field or 3D Photos. Additionally, we avoid the inherent ambiguity due to the aperture problem in stereo cameras by designing the stereo baseline to be orthogonal to the dual-pixel baseline. We present experiments and comparisons with state-of-the-art approaches to show that our method offers a substantial improvement over previous works.",
isbn="978-3-030-58452-8"
}

@ARTICLE{Australia_spiders,
  author={Billinghurst, Mark and Piumsomboon, Tham and Bai, Huidong},
  journal={IEEE Computer Graphics and Applications}, 
  title={Hands in Space: Gesture Interaction with Augmented-Reality Interfaces}, 
  year={2014},
  volume={34},
  number={1},
  pages={77-80},
  doi={10.1109/MCG.2014.8}
  }
  
@inproceedings{markerless_ar,
  title={Markerless visual fingertip detection for natural mobile device interaction},
  author={Baldauf, Matthias and Zambanini, Sebastian and Fr{\"o}hlich, Peter and Reichl, Peter},
  booktitle={Proceedings of the 13th International Conference on Human Computer Interaction with Mobile Devices and Services},
  pages={539--544},
  year={2011}
}

@inproceedings{ar_tabletop,
  title={Virtual object manipulation on a table-top AR environment},
  author={Kato, Hirokazu and Billinghurst, Mark and Poupyrev, Ivan and Imamoto, Kenji and Tachibana, Keihachiro},
  booktitle={Proceedings IEEE and ACM International Symposium on Augmented Reality (ISAR 2000)},
  pages={111--119},
  year={2000},
  organization={Ieee}
}

@inproceedings{fingartips,
  title={FingARtips: gesture based direct manipulation in Augmented Reality},
  author={Buchmann, Volkert and Violich, Stephen and Billinghurst, Mark and Cockburn, Andy},
  booktitle={Proceedings of the 2nd international conference on Computer graphics and interactive techniques in Australasia and South East Asia},
  pages={212--221},
  year={2004}
}

@article{indian_sign_language,
title = {DF-WiSLR: Device-Free Wi-Fi-based Sign Language Recognition},
journal = {Pervasive and Mobile Computing},
volume = {69},
pages = {101289},
year = {2020},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2020.101289},
url = {https://www.sciencedirect.com/science/article/pii/S1574119220301267},
author = {Hasmath Farhana Thariq Ahmed and Hafisoh Ahmad and Kulasekharan Narasingamurthi and Houda Harkat and Swee King Phang},
}

@article{combined_sign_language,
title = {Recognition of user-dependent and independent static hand gestures: Application to sign language},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103193},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103193},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001231},
author = {Khadidja Sadeddine and Fatma Zohra Chelali and Rachida Djeradi and Amar Djeradi and Sidahmed Benabderrahmane},
}

@article{hand_classical_approach,
  title={Vision-based hand interaction in augmented reality environment},
  author={Shen, Yan and Ong, Soh-Khim and Nee, Andrew YC},
  journal={Intl. Journal of Human--Computer Interaction},
  volume={27},
  number={6},
  pages={523--544},
  year={2011},
  publisher={Taylor \& Francis}
}

@article{deep_cnn,
title = {Simple very deep convolutional network for robust hand pose regression from a single depth image},
journal = {Pattern Recognition Letters},
volume = {119},
pages = {205-213},
year = {2019},
note = {Deep Learning for Pattern Recognition},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2017.10.019},
url = {https://www.sciencedirect.com/science/article/pii/S0167865517303872},
author = {Qing Fan and Xukun Shen and Yong Hu and Changjian Yu},
}

@article{ hand_pose_rgb_camera
,
title = {Accurate and efficient 3D hand pose regression for robot hand teleoperation using a monocular RGB camera},
journal = {Expert Systems with Applications},
volume = {136},
pages = {327-337},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.06.055},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419304634},
author = {Francisco Gomez-Donoso and Sergio Orts-Escolano and Miguel Cazorla},
keywords = {Hand pose estimation, Deep learning, Robot teleoperation, Monocular},
}

@article{cnn_finetuning,
title = {A CNN model for real time hand pose estimation},
journal = {Journal of Visual Communication and Image Representation},
volume = {79},
pages = {103200},
year = {2021},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103200},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321001279},
author = {Lu Ding and Yong Wang and Robert Lagani√®re and Dan Huang and Shan Fu},
}


@article{pose_guided_cnn,
title = {Pose guided structured region ensemble network for cascaded hand pose estimation},
journal = {Neurocomputing},
volume = {395},
pages = {138-149},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2018.06.097},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219309087},
author = {Xinghao Chen and Guijin Wang and Hengkai Guo and Cairong Zhang},
keywords = {Hand pose estimation, Convolutional neural network, Human computer interaction, Depth images},
}

@article{hand_pose_occlusions,
title = {Hand pose estimation in object-interaction based on deep learning for virtual reality applications},
journal = {Journal of Visual Communication and Image Representation},
volume = {70},
pages = {102802},
year = {2020},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2020.102802},
url = {https://www.sciencedirect.com/science/article/pii/S1047320320300523},
author = {Min-Yu Wu and Pai-Wen Ting and Ya-Hui Tang and En-Te Chou and Li-Chen Fu},
}

@inproceedings{depth_heatmaps,
  title={Robust 3d hand pose estimation in single depth images: from single-view cnn to multi-view cnns},
  author={Ge, Liuhao and Liang, Hui and Yuan, Junsong and Thalmann, Daniel},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3593--3601},
  year={2016}
}

@inproceedings{deep_pose,
  title={Deeppose: Human pose estimation via deep neural networks},
  author={Toshev, Alexander and Szegedy, Christian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1653--1660},
  year={2014}
}

@inproceedings{resnet_50,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{yolo_9000,
  title={YOLO9000: better, faster, stronger},
  author={Redmon, Joseph and Farhadi, Ali},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7263--7271},
  year={2017}
}

@inproceedings{zfnet,
  title={Visualizing and understanding convolutional networks},
  author={Zeiler, Matthew D and Fergus, Rob},
  booktitle={European conference on computer vision},
  pages={818--833},
  year={2014},
  organization={Springer}
}

@inproceedings{ransacpointcloud,
  title={Efficient RANSAC for point-cloud shape detection},
  author={Schnabel, Ruwen and Wahl, Roland and Klein, Reinhard},
  booktitle={Computer graphics forum},
  volume={26},
  number={2},
  pages={214--226},
  year={2007},
  organization={Wiley Online Library}
}

@inproceedings{snaptoreality,
  title={Snaptoreality: Aligning augmented reality to the real world},
  author={Nuernberger, Benjamin and Ofek, Eyal and Benko, Hrvoje and Wilson, Andrew D},
  booktitle={Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
  pages={1233--1244},
  year={2016}
}

@inproceedings{liu2019planercnn,
  title={Planercnn: 3d plane detection and reconstruction from a single image},
  author={Liu, Chen and Kim, Kihwan and Gu, Jinwei and Furukawa, Yasutaka and Kautz, Jan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4450--4459},
  year={2019}
}

@inproceedings{yang2010plane,
  title={Plane detection in point cloud data},
  author={Yang, Michael Ying and F{\"o}rstner, Wolfgang},
  booktitle={Proceedings of the 2nd int conf on machine control guidance, Bonn},
  volume={1},
  pages={95--104},
  year={2010}
}