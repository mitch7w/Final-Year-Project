\chapter[2022 March]{March 2022}

\section[2022/03/30]{Wednesday, 30 March 2022}
\label{sec:20220330}
\subsection{Background reading and research}
The construction of a system that can both receive input in the form of human gestures and can output a video stream of a virtual object in augmented reality from a separate video input stream has a number of components. In terms of software, these are the human gesture recognition system, virtual object creation system and output video synthesis system. In terms of hardware two webcams or other form of camera should be sufficient for gesture input and environment input. \newline

Background research was conducted on depth measurement and how to conduct it in embedded environments. The use of ultrasonic emitters  and receivers seems to be the principal method used in hobbyist and embedded applications. In more serious academic implementations the software approach of computational stereo has been taken that compares the input of two closely-placed cameras and uses triangulation between them and a pixel of an object to estimate depth of objects in the environment. Du2Net uses modern smartphones with multiple cameras and dual-sensor pixels to compute depth. \cite{Du2Net} Serious business endeavours use LIDAR sensors and radar to create three-dimensional maps of the environment around them. Certain industry implementations of depth sensing such as \href{https://www.tesla.com/autopilot}{Tesla Autopilot} rely almost entirely on computer vision using regular cameras and a small host of ultrasonic sensors to construct a representation of the environment around the car. This approach requires massive neural networks that can match training image data to training depth data and thus estimate depth reliably using computer vision only. \newline

Depending on the kind of implementation required it may be necessary to use hardware such an ultrasonic sensor to estimate depth and the surroundings of the environment the virtual object must be placed in, or it may be able to be done entirely using computational computer vision approaches. 

There are a few online tutorials and online projects that perform facetracking and object recognition using hobbyist boards like Arduino but it is actually a computer that performs the computer vision and then just sends instructions to a servo motor or actuator connected to the microcontroller. Actual computer vision requires computation that isn't available on small embedded platforms like Arduino boards. There are thus limited options to chose from that suit a budget of R3000 and allow embedded video processing - this is the critical decision, what hardware will allow this system to be constructed such that it performs adequately in real time - the most important operating characteristic. \newline

Google's \href{https://developers.google.com/ar/develop}{ARCore} is a software development kit that allows augmented reality applications to be built on many platforms. It contains three main capabilities - motion tracking, environmental understanding of surfaces and light estimation. It conducts many of the high-level tasks needed to implement the system described in the project proposal and is an off-the-shelf solution that should be investigated for inspiration/technical direction.

A point cloud is a set of data points in space that represents an object or environment on a three-dimensional Cartesian grid. It can be used for surface reconstruction which builds a model of a set of surfaces in memory or for terrain reconstruction - basically three-dimensional topographical maps of an environment. Constructing a point cloud of the environment the virtual object is to be placed within would be a helpful first step as the correct scaling and placement of a virtual object could then be computed using those point cloud depth datapoints. Online construction of a three-dimensional model from point maps is difficult as this requires a lot of computation and several industry implementations still use offline training - just because it is cheaper and more accurate. \href{https://google-cartographer-ros.readthedocs.io/en/latest/}{Google Cartographer} uses online training but requires at least a high-end Intel i7 chip to run the required algorithms. Potentially if the resolution and quality of the model is reduced it could be done on an embedded platform and there are examples of this such as \href{https://play.google.com/store/apps/details?id=com.rovio.abar&hl=en_ZA&gl=US}{Angry Birds AR} on smartphones. \newline

\section[2022/03/31]{Thursday, 31 March 2022}

\subsection{Prototype ideas}

A list of prototype task ideas that could aid in deciding on further specifications for the system to built are as follows:
\begin{compactitem}
    \item A small image recognition system that could identify at least 1 distinct hand gesture on a small embedded platform like an ESP32
    \item Depth point cloud creation of 10 points using a microcontroller like an Arduino with a hobbyist ultrasonic sensor
    \item Generation of a shape onto a JPEG on a PC
\end{compactitem}

These tasks would aid in deciding on further specifications because it would be able to estimate how much processing power is needed to run a computer vision application that can correctly interpret hand gestures, how much memory it takes to store a depth point cloud as well if a physical rotating device is needed to move the ultrasonic sensor around to accurately map a scene - like a LIDAR sensor. These tasks might even be impossible as the processing power of these hobbyist electronic devices are really limited (ESP32 has a max of 240MHz clock) and so even attempting them will inform the design decisions to be elucidated in the project proposal. Hobbyist boards are suggested here simply because they are available and affordable and do not impact the project's budget. It is also uncertain if a more expensive LIDAR or Microsoft Kinect-like sensor is needed to accurately interpret the depth of an environment or if computer vision-only approaches will yield enough information for the system to operate accurately. Finally it is also imperative to see the graphical computations involved in drawing a shape over a still image and predicting how that would scale to processing images say at 30 frames per second required to insert a virtual object into a real live video feed.\newline

\subsection{Literature Analysis}

The process of identifying hand gestures has been well studied and generally consists of a number of steps. In the hand gesture recognition system built by Shen et al. \cite{hand_ar_shen} the process includes segmenting the region of the image that just contains a hand from the background imagery, finding and then keeping track of certain components of the hand such as fingertips and the deformations between the bottom of the fingers (convexity defect points) as well as then determining the relative position and orientation of the detected hand to the camera capturing the image to estimate where and at what scale an object to be virtually created should be rendered. \newline

Hand segmentation in \cite{hand_ar_shen} is conducted using nonparametric skin color modelling in order to perform well for multiple different skin colors and in varying light conditions. The Continously Adaptive Mean Shift algorithm is used for this by recognizing very high or very low saturation of image pixels and since human skin hues are all very similar will be able to pick them out from background imagery. The same study uses a curvature-based algorithm for detecting features of the hand such as fingertips. This encompasses calculating vectors on the hand pixels based on the hand contours and a least-square fitting method. If the system running the hand segmentation and detection process is fast enough the entire process can be repeated for each frame of video to be analyzed but if it is not, tracking and predicting where in the next frame the points of the hand will be is required. Shen et al. utilize a "P4 3 GHz PC equipped with 1 GB RAM" to run their hand gesture recognition algorithm. 

A slightly different approach is taken by University Of Canterbury's Human Interface Technology Lab hand gesture control system \cite{canterbury_hand_ar} where a Microsoft Kinect is used to reconstruct the surface of a tabletop using point cloud information from the Kinect and allows virtual objects to interact with human hands detected by skin-colored regions of the image. The resulting virtual object is displayed either on an Android mobile phone or desktop PC. \newline

Other approaches include utilizing depth-sensing hardware such as a Leap Motion sensor to detect depth and construct a hand skeletal model \cite{kim_leap_ar} but this implementation is off-the-shelf in that the hand tracking is performed by the Leap motion itself and outputs a live model of the user's hands from the sensor. In Lee et al.'s implementation of gesture control of an object \cite{lee_hand_ar} a standard camera is used to predict a six-degree-of-freedom camera pose also with a similar skin-color segmentation algorithm that can identify fingertips and thus build a virtual model of the hand. Their implementation includes optical flow tracking calculations and multithreading for predicting future hand positions based on current ones - a useful solution to a noisy environment or under-powered system. \newline

Several papers allude to the seminal "Statistical Color Models with Application to Skin Detection" paper \cite{jones_skin_color} that outlines how different human skin color is very different in intensity or saturation but not hue. Hue is a pure color and intensity is the saturation/brightness of a colour. Thus skin and non-skin colors can be mapped into a histogram model of skin and non-skin pixel colors which can then be applied in the segmentation step to differentiate skin versus non-skin pixels in an image. It is undetermined if the data for a skin classifier must be gathered from first principles or if it can be used off-the-shelf. \newline

The use of hand gestures to control other devices has even been extended to the control of drones \cite{drones_hololens}. This is notable because it operates in real-time and on an embedded albeit expensive platform - the Microsoft Hololens. This system uses four RGB cameras as well as a time-of-flight sensor to track and interpret the user's hand gestures. The Hololens contains a Qualcomm Snapdragon 850 mobile CPU and points to the similar conclusion as the other papers discussed above that the computation of augmented reality applications relies on still computationally heavy processors - not cheap embedded systems. The drone system performs gesture control by analyzing the hand joint's position, angles between the fingers and "co-linearity between finger pairs" to determine the current state of the operator's hand and interprets these as commands for the drone. The system ran video processing on the Hololens device and with custom FFmpeg decoding tools to process frames of video. \newline

Researchers at Google have used \href{https://google.github.io/mediapipe/solutions/hands}{MediaPipe} and in-house developed algorithms to accurately track the hand and gestures of a user using just an RGB camera. \cite{mediapipe_hands} Their two main algorithms include a palm detector which segments the hand from the rest of the image and a hand landmark model which predicts the shape of the hand skeleton based on the segmented image and predicts the position of 21 hand-knuckle points. Their solution trains the hand skeleton classifier on a dataset of thousands of images of hands in various gestures with the skeleton model annotated on the image. This is a more deep-learning approach than detailed segmentation and finger convexity calculations like in the Shen et al. study \cite{hand_ar_shen}. The Google researchers produce smaller and more lightweight versions of their model for use on smartphones and web applications as well. \newline

\subsection{Impact on system to be developed}

The literature studied at this point points to a number of conclusions for the system to be developed. Firstly, the hand gesture recognition algorithm will require building some kind of model of a hand if anything more than a few discrete gestures are to be identified by the system and if continuous control of an object is to be implemented. A hand skeleton model or some other Cartesian coordinate system needs to be mapped onto a detected hand and live-updated to represent gesture input. Secondly, in order to build a system that intelligently recognizes the scale and depth of an environment presented to the system via video input some form of depth measurement will need to be taken of the environment. Certain applications operate on vision-only input but these are computationally expensive and not as accurate or reliable on embedded hardware as applications that combine regular RGB camera input with either time-of-flight or ultrasonic depth data of an environment. Thirdly, the computation of all of these algorithms will be extremely heavy if the system is to operate in real time and thus either the system needs to perform its computation on a PC with respectable specifications for graphics and matrix mathematics or the subcomponents of the system need to operate on different embedded platforms - one device for the hand gesture recognition algorithms and one device for the environment detection, graphical creation and combination algorithms. 