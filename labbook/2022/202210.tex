\chapter[2022 October]{October 2022}

\section[2022/10/03]{Monday, 03 October 2022}

\subsection{Final version of literature review}

The literature review has been expanded from the original two pages presented in the first semester progress report to five pages. It is presented below for continuity and backup's sake. \\

With the increase in the proliferation of powerful personal computing hardware it has become feasible to create augmented reality applications that integrate virtual objects with a user’s physical environment. Similarly, modern computer systems can perform real-time inference on a large range of alternative inputs and return useful results – this has led to the advent of human-control inputs to computers like hand gesture control using regular webcams. These two sub-fields - augmented reality and gesture control, can be combined to give a user a natural and intuitive control mechanism for interactive and visual applications. \\

The literature is studded with examples of applications that use this combination of technologies, such as Billinghurst \cite{Australia_spiders} who utilizes a Microsoft Kinect depth and RGB camera to treat agoraphobia by creating virtual spiders that the user can interact with using their hands in an augmented reality application. This is accomplished by extracting point cloud depth and RGB camera data of a table and the user's hands and segmenting the hands out from the background using the point cloud depth information from the Kinect sensor. The virtual spiders are then overlaid on the user's hands in software and displayed on a desktop monitor - creating the illusion of augmented reality. The same researchers also utilize the Kinect sensor to reconstruct virtual cars on a tabletop using surface reconstruction and enable interactions between those virtual cars and real-world objects by use of a virtual mesh that is updated in real-time as the real-world objects are moved around.\\

Collisions between the virtual cars and real-world objects are detected by checking if the hands and virtual mesh are in the same position or have the same depth data values. Additionally the system locates the user's hands by segmenting the RGB image from the Kinect sensor by skin color and a curve-finding algorithm is used to locate the fingertips - a rudimentary but effective solution to hand tracking if the background environment is sufficiently devoid of noise. \\

Baldauf \cite{markerless_ar} uses gesture input from a mobile phone camera to select, shrink and zoom in on virtual objects presented in the environment as well as to recognize gesture volume controls for a music application. The system makes use of a skin detection algorithm to create a binary image representing the hand which is then de-noised and supplied to an OpenCV algorithm to find the hand's contours. From these contours the palm of the hand is found by finding the largest circle that can fit inside of the segmented hand. Following this, a distance algorithm is used that finds curves a certain distance away from the palm to detect the fingertips of the hand. The location of the fingertips and the distance between the index finger and thumb is then used to either grow or shrink a virtual cube superimposed on the image or increase and decrease the volume of a music player application - real-world use cases for applications that rely on gesture input. \\

The ability to locate virtual objects in the context of the real-world environment in an augmented reality application is important if realistic interaction is to take place. Kato \cite{ar_tabletop} implements a tabletop augmented reality application for handling small virtual shapes and cards that relies upon a global coordinate system and paper tracking fiducials placed on the tabletop to give both virtual objects and real-world objects their coordinates in the global coordinate system and then be able to control virtual object movement and behavior accordingly. The global coordinate system is defined relative to the paper fiducials placed on the table and the camera's position determined from triangulating its distance to each fiducial. Furthermore, the system allows for rotation and tilting of the virtual shapes by comparing current global coordinates of a virtual shape to previous coordinates and if the difference is great enough rotating or tilting the shape in the correct direction. \\

Similarly, Buchmann \cite{fingartips} uses a world coordinate system in an urban planning augmented reality application that tracks the position of virtual objects as well as the user’s hand and current gesture to determine if an object should be grasped, moved or released at any given time. This also allows for collision avoidance as the same coordinate system is shared by all objects – real or virtual. The system also relies upon paper fiducial markers placed on the tabletop and on a glove that the user dons for input to the world coordinate system. A real-time model of the user's hand and the current gesture is created from the orientation of the fiducial markers in relation to a camera. \\

Since the fiducial markers on the glove and tabletop all share the same world coordinate system they can be easily compared and allows the user to interact seamlessly with the virtual buildings and roads rendered in the urban planning augmented reality application. The current gesture of the hand is recognizing by comparing the distance of the fingertip fiducial markers from each other and by checking if the fingertips are a certain distance inside or away from virtual objects' positions. Thus simple gestures such as grabbing, releasing and dragging can be recognized. Small electronic buzzers that vibrate when the user touches or drags an object are used to provide the user with haptic feedback when using the application. \\

These applications receive hand gestures as input and hand gesture control itself can be considered as the two sequential problems of hand pose estimation and gesture recognition based on the hand pose predicted. Gesture recognition is either performed using the classical approaches described above that centre around hand contour-finding or fiducial marker detection, or as is more common in recent approaches, is performed using machine learning approaches such as support vector machines, Naïve-Bayes classifiers and convolutional neural networks as by Ahmed \cite{indian_sign_language} for the recognition of Indian sign language based on hand coordinate input. It can also be accomplished by extracting features from the input image using Gabor Wavelet Transforms and gradient local-auto correlation and then providing these features to a multi-layer perceptron or K-nearest neighbors system such as by Sadeddine \cite{combined_sign_language} to recognize sign language. The complexity of the algorithm required in gesture recognition depends on the static or dynamic nature as well as diversity of the input gestures. \\

What is apparent from the literature, however, is that the main challenge of gesture recognition is first acquiring an estimation of the user’s hand pose from camera input – solutions to this problem have been proposed and implemented since the 1990s. These early solutions \cite{hand_classical_approach} relied on classical approaches to hand pose estimation such as using The Continuously Adaptive Mean Shift algorithm to recognize the very high or low saturation of image pixels in the Hue, Saturation and Value colourspace (HSV) to segment a hand from its background and then using a curvature-based least-square fitting algorithm for detecting the contours of the hand such as fingertips. Additional information about the hand pose is estimated from the contours of the palm and used to find the convexity defect points between fingertips - these allow the orientation of the hand relative to the camera to be found. \\

An alternative to hand pose estimation is to use a physical glove with fiducial markers on it as used by Buchmann \cite{fingartips} to detect the position of a user’s hand in space. However, with the advent of modern computing power and the rise of deep learning, the literature has been saturated with machine learning approaches to hand pose estimation that require none of the special hardware or highly specific algorithms that previous implementations required. \\

A state-of-the-art hand-tracking application created by Google - dubbed Mediapipe Hands \cite{mediapipe_hands}, uses a series of convolutional neural networks to train a palm detector and hand landmark model to output coordinates of hand joint landmarks. The system runs in real-time on mobile devices and is trained using real images of hands as well as synthetic hand models. In order to reduce the complexity that the hand joint coordinate neural network must deal with, a palm detector is first implemented using a single-shot detector to predict a bounding box around the palm present in the input image.\\

Once the palm has been detected, the image is cropped to this bounding box and a convolutional pose machine is used to create a confidence map that shows the probability of finding a fingertip at any given point in the input image. This convolutional pose machine is comprised mainly of simple convolutional and pooling layers. From the generated confidence maps, the system outputs the x,y and relative z coordinates of the 21 hand landmarks which together represent an accurate depiction of the current pose of the user's hand. \\

Similarly, Qing \cite{deep_cnn} uses a deep convolutional neural network with just convolutional and pooling layers to output three-dimensional joint locations for a hand based on depth image input. This is why the literature often refers to hand pose estimation as hand joint-regression. The advantage of the very deep convolutional neural network is that it obviates many of the intermediate feature extraction tasks that would otherwise have to be designed by hand and instead allows the system to learn these itself. \\

The architecture of the neural network used by Qing relies upon eight convolutional layers, four pooling layers and three fully-connected layers at the output of the system. This is aided by batch normalization and allows the system to take in a simple depth image and regress all the way to coordinates for the various landmarks of the hand. Gomez-Donoso \cite{hand_pose_rgb_camera} employs a similar architecture to predict joint locations by first using a convolutional neural network to detect and segment the hand using a box prediction system reminiscent of the YOLO9000 architecture \cite{yolo_9000}, and then regress the joints of the hand using a large convolutional neural network based on the RESNET50 architecture \cite{resnet_50}.\\

Specifically, the system uses a convolutional neural network to detect the probability of a hand being present in various regions of the image. This is accomplished using an object localization or box prediction for the hand that uses a smaller version of the full YOLO9000 architecture - nineteen convolutional layers and five maxpooling layers. Once the hand has been detected the cropped image of the hand is passed to a modified RESNET50 convolutional neural network that is adapted to regress normalized 3D hand joint coordinates from the simple cropped input image of the hand. The two neural networks are trained using a combination of existing weights and a custom dataset of hand poses taken with a Leap Motion depth and RGB sensor. \\

Alternatively, there are implementations of hand pose estimation that make exclusive use of depth camera input. This depth input is often modified in an intermediate transformation such as a heatmap to show where each joint of the hand is likely to be and regresses the location of the joints from this intermediate layer. This is the approach taken by Chen \cite{pose_guided_cnn} where a convolutional neural network regresses joint locations from feature regions which are themselves extracted from feature heatmaps created by depth image input. \\

Specifically, the system built by Chen takes in a depth image and rough previous hand pose estimation. The depth image is run through a small convolutional neural network (six convolutional layers and two residual connections) that outputs feature heatmaps for each joint of the hand which are used in combination with the previous rough hand pose estimation to extract feature regions for each joint of the hand. These feature regions are then hierarchically connected to a final convolutional neural network which outputs the regressed coordinates for various joints of the hand and represents the hand pose estimation of the entire system. \\

Ding \cite{cnn_finetuning} and Ge \cite{depth_heatmaps} also make use of heatmaps of joint coordinates and subsequent fine-tuning algorithms to output joint locations based on the intermediate layers. Ding \cite{cnn_finetuning} simplifies segmentation of the hand from the background by assuming that the hand is the closest object to the camera and by using a depth camera, extracts a fixed region of depth and RGB information from around the closest depth value to the camera. This extracted region is then resized and fed into a small convolutional neural network that outputs hand pose parameters which are used with another hand model layer (featuring six convolutional layers) to regress rough estimates for the hand joint locations. Fine-tuning is then performed to modify the output of this layer by giving larger weight to predictions that match up with the initial rough hand pose estimation and reducing the weight given to predictions made away from the hand's centre as those are more regularly inaccurate. Coupled with many rotations and translations in a data augmentation process, the system is used to accurately provide an estimate of hand poses from RGB and depth image input in real-time. \\

Ge \cite{depth_heatmaps} takes an input depth image and projects it onto three different orthogonal planes - the  x-y, y-z and z-x planes of a bounding box around the image. Separate convolutional networks are then used to take the depth projections described above and output feature maps which when combined through a final fully-connected layer, output a heatmap showing the probability of a hand joint being present at each coordinate - giving an accurate hand pose estimation from multiple three-dimensional views. \\

Wu \cite{hand_pose_occlusions} develops an architecture that involves calculating a skeleton-difference loss network to regress the joints of a hand skeleton based on depth camera input. The system works by accepting depth images as input and uses a ZF-Net \cite{zfnet} inspired convolutional neural network to generate bounding boxes for a detected hand in the input image. The image is then cropped to this bounding box and passed to the skeleton-difference loss neural network. This network seeks to minimize the angle between all the joints of the hand skeleton and between the joint length and ground truth joint length. This network predicts the location of the hand joints using a one-hundred-and-one layer recurrent neural network. Additionally, this implementation is developed to be robust to occlusions of the hand by objects held in the hand.\\

Hand pose estimation itself is a sub-field of full-body pose estimation which is a problem solved by Toshev \cite{deep_pose} and which takes advantage of a hierarchical progression of increasingly-fine-grained pose regressors for the joints of a whole body and is comprised at its core of simple convolutional layers stacked after one another. \\

It is evident that the advent of deep learning has yielded a large number of new approaches to hand pose estimation and that the extensive use of convolutional neural networks is the modern approach most preferred in academia. This is due to the ease of not having to implement detailed representations of low-level hand shapes, patterns and methods of identifying these features in input imagery but rather instead training a deep learning system to identify and learn these low-level abstractions using vast amounts of training data and optimized architectures such as the convolutional neural network. \\

The majority of hand pose estimation systems surveyed above rely on some form of detection of the user's hand or palm as a provisional step to hand pose estimation. This is because detecting the hand allows a system to crop the input RGB or depth image to only those dimensions that include a hand and allow background noise and interference to be suppressed - increasing the accuracy of hand pose estimation and hand joint localisation systems as well as decreasing the computational complexity and training time needed to train these large neural network systems to be robust against noise and translation. The classical approaches to hand detection and fingertip detection mainly rely on skin color or hue segmentation followed by contour and curvature-based algorithms that can detect the contours and curves of the user's hand and locate these in a segmented input image - however these have their shortcomings when it comes to noise tolerance and reliability. \\

Many augmented reality applications have been created that rely on hand and gesture input and all use some form of global or world coordinate system shared by real-world and virtual objects. This allows the position of these objects to be related to each other and for collisions and interactions between these objects to be modelled and rendered. Classically, paper fiducial markers were used to orient the camera and synchronise virtual as well as real-world objects to the global coordinate system however modern approaches have dispensed with these as depth-tracking devices such as the Microsoft Kinect and Leap Motion sensors have become available for academic use and can fulfill much the same purpose. \\

Considering the broad body of literature on hand gesture control of virtual objects and their applicability to augmented reality applications, several design choices have been informed for the system to be implemented. It is evident that the preferred approach in the literature for hand pose estimation and gesture recognition is to use a deep-learning architecture to regress hand joint coordinates from either a depth or standard RGB camera input. Gesture recognition can either be performed by deep-learning approaches or by classical calculations depending on the complexity of the required gestures. Augmented reality and the combination of virtual reality objects with real-world objects can create immersive and useful applications when a suitable input camera is used and a shared coordinate system is established to track both virtual and real objects and prevent collisions between them. \\

The use of large-scale neural networks for gesture recognition and joint regression is mainly used for detecting large numbers of gestures or finding the coordinates of tens of hand joints in an input image. Scaling down the output of these networks can significantly reduce the complexity needed in their construction and in the amount of layers needed for accurate operation. Thus, designing a system that only differentiates between a handful of gestures or localizes one or two hand landmarks will be much cheaper to develop computationally and allow better operation using an embedded device and first-principles algorithms. \\

The additional use of pre-processing using skin color and hue segmentation, depth data as well as hand detection and bounding-box algorithms - whether classical or using a deep-learning approach, will also greatly reduce the complexity of the gesture recognition and hand pose estimation design work to be completed. In conclusion, a system will be developed that can accept user gesture input using a deep-learning approach coupled with targeted pre-processing and then translate that gesture into meaningful instructions for a virtual object present in an augmented reality scene that presents realistic interactions between it and real-world objects and uses a global coordinate system to prevent object collisions and model realistic interactions. \\

\section[2022/10/10]{Monday, 10 October 2022}

\subsection{Revised Final version of literature review}

Additional information pertaining to plane estimation using depth data was added to the literature review. This was done per Mr. Grobler's instructions at the final individual progress meeting last week. 

With the increase in the proliferation of powerful personal computing hardware it has become feasible to create augmented reality applications that integrate virtual objects with a user’s physical environment. Similarly, modern computer systems can perform real-time inference on a large range of alternative inputs and return useful results – this has led to the advent of human-control inputs to computers like hand gesture control using regular webcams. These two fields - augmented reality and gesture control, can be combined to give a user a natural and intuitive control mechanism for interactive and visual applications. \\

The literature is studded with examples of applications that use this combination of technologies, such as Billinghurst \cite{Australia_spiders} who utilizes a Microsoft Kinect depth and RGB camera to treat agoraphobia by creating virtual spiders that the user can interact with using their hands in an augmented reality application. This is accomplished by extracting point cloud depth and RGB camera data of a table and the user's hands and segmenting the hands out from the background using the point cloud depth information from the Kinect sensor. The virtual spiders are then overlaid on the user's hands in software and displayed on a desktop monitor - creating the illusion of augmented reality. The same researchers also utilize the Kinect sensor to reconstruct virtual cars on a tabletop using surface reconstruction and enable interactions between those virtual cars and real-world objects by use of a virtual mesh that is updated in real-time as the real-world objects are moved around.\\

Collisions between the virtual cars and real-world objects are detected by checking if the hands and virtual mesh are in the same position or have the same depth data values. Additionally, the system locates the user's hands by segmenting the RGB image from the Kinect sensor by skin color and a curve-finding algorithm is used to locate the fingertips - a rudimentary but effective solution to hand tracking if the background environment is sufficiently devoid of noise. \\

Baldauf \cite{markerless_ar} uses gesture input from a mobile phone camera to select, shrink and zoom in on virtual objects presented in the environment as well as to recognize gesture volume controls for a music application. The system makes use of a skin detection algorithm to create a binary image representing the hand which is then de-noised and supplied to an OpenCV algorithm to find the hand's contours. From these contours the palm of the hand is found by finding the largest circle that can fit inside of the segmented hand. Following this, a distance algorithm is used that finds curves a certain distance away from the palm to detect the fingertips of the hand. The location of the fingertips and the distance between the index finger and thumb is then used to either grow or shrink a virtual cube superimposed on the image or increase and decrease the volume of a music player application - real-world use cases for applications that rely on gesture input. \\

The ability to locate virtual objects in the context of the real-world environment in an augmented reality application is important if realistic interaction is to take place. Kato \cite{ar_tabletop} implements a tabletop augmented reality application for handling small virtual shapes and cards that relies upon a global coordinate system and paper tracking fiducials placed on the tabletop to give both virtual objects and real-world objects their coordinates in the global coordinate system and then be able to control virtual object movement and behavior accordingly. The global coordinate system is defined relative to the paper fiducials placed on the table and the camera's position is determined from triangulating its distance to each fiducial. Furthermore, the system allows for rotation and tilting of the virtual shapes by comparing current global coordinates of a virtual shape to previous coordinates and if the difference is great enough rotating or tilting the shape in the correct direction. \\

Similarly, Buchmann \cite{fingartips} uses a world coordinate system in an urban planning augmented reality application that tracks the position of virtual objects as well as the user’s hand and current gesture to determine if an object should be grasped, moved or released at any given time. This also allows for collision avoidance as the same coordinate system is shared by all objects – real or virtual. The system also relies upon paper fiducial markers placed on the tabletop and on a glove that the user dons for input to the world coordinate system. A real-time model of the user's hand and the current gesture is created from the orientation of the fiducial markers in relation to a camera. \\

Since the fiducial markers on the glove and tabletop all share the same world coordinate system they can be easily compared and allows the user to interact seamlessly with the virtual buildings and roads rendered in the urban planning augmented reality application. The current gesture of the hand is recognizing by comparing the distance of the fingertip fiducial markers from each other and by checking if the fingertips are a certain distance inside or away from virtual objects' positions. Thus simple gestures such as grabbing, releasing and dragging can be recognized. Small electronic buzzers that vibrate when the user touches or drags an object are used to provide the user with haptic feedback when using the application. \\

Augmented reality applications also sometimes depend upon plane detection to identify surfaces depending on their application. Schnabel \cite{ransacpointcloud} utilizes the Random Sample Consensus (RANSAC) algorithm to search through unstructured point clouds of depth information and output shapes that demonstrate where planar surfaces exist in the depth data. This works by calculating the normal vectors for randomly selected pixels in the point cloud and growing a region that contains all similar normal vectors until no more similar vectors can be found and then adding that region and its shape to an existing array of known planar surfaces - while removing the pixels from the point cloud data. the In this way, planar surfaces can be found efficiently in depth point clouds and virtual reality objects placed on those detected surfaces. \\

This is also partially the approach taken by Nuernberger \cite{snaptoreality} where a Microsoft Kinect is used to capture depth data. Following this, the data is filtered using an exponential filter and surface normal vectors are computed at each pixel and then used to detect edges of objects in the depth point cloud. From these edges, the Hough Transform is used to find dominant lines and the RANSAC algorithm to find points inside the edges of those lines. The Hough Transform is another widely-used algorithm for plane detection and works by finding all the planes each point in a point cloud can fall on and then using a data structure called an accumulator to iteratively find the planes that contain the most points in the point cloud. In this way the dominant planes present in the point cloud can be estimated. \\

The application by \cite{snaptoreality} goes on to estimate planar surfaces from the extracted edges and project virtual shapes onto the surfaces. Other applications such as by Liu \cite{liu2019planercnn} utilize convolutional neural networks to estimate planar surfaces but the industry standard is to use some version of RANSAC for simple applications due to its insensitivity to noise and overall simplicity, such as by Yang \cite{yang2010plane} with the combination of RANSAC and MDL to find planes in unstructured point cloud depth data.

All of the applications that use hand gestures as input and hand gesture control itself can be considered as solving the two sequential problems of hand pose estimation and gesture recognition based on the hand pose predicted. Gesture recognition is either performed using the classical approaches described above that centre around hand contour-finding or fiducial marker detection, or as is more common in recent approaches, is performed using machine learning approaches such as support vector machines, Naïve-Bayes classifiers and convolutional neural networks as by Ahmed \cite{indian_sign_language} for the recognition of Indian sign language based on hand coordinate input. It can also be accomplished by extracting features from the input image using Gabor Wavelet Transforms and gradient local-auto correlation and then providing these features to a multi-layer perceptron or K-nearest neighbors system such as by Sadeddine \cite{combined_sign_language} to recognize sign language. The complexity of the algorithm required in gesture recognition depends on the static or dynamic nature as well as diversity of the input gestures. \\

What is apparent from the literature, however, is that the main challenge of gesture recognition is first acquiring an estimation of the user’s hand pose from camera input – solutions to this problem have been proposed and implemented since the 1990s. These early solutions \cite{hand_classical_approach} relied on classical approaches to hand pose estimation such as using The Continuously Adaptive Mean Shift algorithm to recognize the very high or low saturation of image pixels in the Hue, Saturation and Value colourspace (HSV) to segment a hand from its background and then using a curvature-based least-squares fitting algorithm for detecting the contours of the hand such as fingertips. Additional information about the hand pose is estimated from the contours of the palm and used to find the convexity defect points between fingertips - these allow the orientation of the hand relative to the camera to be found. \\

An alternative to hand pose estimation is to use a physical glove with fiducial markers on it as used by Buchmann \cite{fingartips} to detect the position of a user’s hand in space. However, with the advent of modern computing power and the rise of deep learning, the literature has been saturated with machine learning approaches to hand pose estimation that require none of the specialised hardware or highly specific algorithms that previous implementations required. \\

This is visible in a state-of-the-art hand-tracking application created by Google - dubbed Mediapipe Hands \cite{mediapipe_hands}, which uses a series of convolutional neural networks to train a palm detector and hand landmark model to output coordinates of hand joint landmarks. The system runs in real-time on mobile devices and is trained using real images of hands as well as synthetic hand models. In order to reduce the complexity that the hand joint coordinate neural network must deal with, a palm detector is first implemented using a single-shot detector to predict a bounding box around the palm present in the input image.\\

Once the palm has been detected, the image is cropped to this bounding box and a convolutional pose machine is used to create a confidence map that shows the probability of finding a fingertip at any given point in the input image. This convolutional pose machine is comprised mainly of simple convolutional and pooling layers. From the generated confidence maps, the system outputs the x,y and relative z coordinates of the 21 hand landmarks which together represent an accurate depiction of the current pose of the user's hand. \\

Similarly, Qing \cite{deep_cnn} uses a deep convolutional neural network with just convolutional and pooling layers to output three-dimensional joint locations for a hand based on depth image input. This is why the literature often refers to hand pose estimation as hand joint-regression. The advantage of the very deep convolutional neural network is that it obviates many of the intermediate feature extraction tasks that would otherwise have to be designed by hand and instead allows the system to learn these itself. \\

The architecture of the neural network used by Qing relies upon eight convolutional layers, four pooling layers and three fully-connected layers at the output of the system. This is aided by batch normalization and allows the system to take in a simple depth image and regress all the way to coordinates for the various landmarks of the hand. Gomez-Donoso \cite{hand_pose_rgb_camera} employs a similar architecture to predict joint locations by first using a convolutional neural network to detect and segment the hand using a box prediction system reminiscent of the YOLO9000 architecture \cite{yolo_9000}, and then regress the joints of the hand using a large convolutional neural network based on the RESNET50 architecture \cite{resnet_50}.\\

Specifically, the system uses a convolutional neural network to detect the probability of a hand being present in various regions of the image. This is accomplished using an object localization or box prediction for the hand that uses a smaller version of the full YOLO9000 architecture - nineteen convolutional layers and five maxpooling layers. Once the hand has been detected, the cropped image of the hand is passed to a modified RESNET50 convolutional neural network that is adapted to regress normalized 3D hand joint coordinates from the simple cropped input image of the hand. The two neural networks are trained using a combination of existing weights and a custom dataset of hand poses taken with a Leap Motion depth and RGB sensor. \\

Alternatively, there are implementations of hand pose estimation that make exclusive use of depth camera input. This depth input is often modified in an intermediate transformation such as a heatmap to show where each joint of the hand is likely to be and regresses the location of the joints from this intermediate layer. This is the approach taken by Chen \cite{pose_guided_cnn} where a convolutional neural network regresses joint locations from feature regions which are themselves extracted from feature heatmaps created by depth image input. \\

Specifically, the system built by Chen takes in a depth image and rough previous hand pose estimation. The depth image is run through a small convolutional neural network (six convolutional layers and two residual connections) that outputs feature heatmaps for each joint of the hand which are used in combination with the previous rough hand pose estimation to extract feature regions for each joint of the hand. These feature regions are then hierarchically connected to a final convolutional neural network which outputs the regressed coordinates for various joints of the hand and represents the hand pose estimation of the entire system. \\

Ding \cite{cnn_finetuning} and Ge \cite{depth_heatmaps} also make use of heatmaps of joint coordinates and subsequent fine-tuning algorithms to output joint locations based on the intermediate layers. Ding \cite{cnn_finetuning} simplifies segmentation of the hand from the background by assuming that the hand is the closest object to the camera and by using a depth camera, extracts a fixed region of depth and RGB information from around the closest depth value to the camera. This extracted region is then resized and fed into a small convolutional neural network that outputs hand pose parameters which are used with another hand model layer (featuring six convolutional layers) to regress rough estimates for the hand joint locations. Fine-tuning is then performed to modify the output of this layer by giving larger weight to predictions that match up with the initial rough hand pose estimation and reducing the weight given to predictions made away from the hand's centre as those are more regularly inaccurate. Coupled with many rotations and translations in a data augmentation process, the system is used to accurately provide an estimate of hand poses from RGB and depth image input in real-time. \\

Ge \cite{depth_heatmaps} takes an input depth image and projects it onto three different orthogonal planes - the  x-y, y-z and z-x planes of a bounding box around the image. Separate convolutional networks are then used to take the depth projections described above and output feature maps which when combined through a final fully-connected layer, output a heatmap showing the probability of a hand joint being present at each coordinate - giving an accurate hand pose estimation from multiple three-dimensional views. \\

Taking a different approach, Wu \cite{hand_pose_occlusions} develops an architecture that involves calculating a skeleton-difference loss network to regress the joints of a hand skeleton based on depth camera input. The system works by accepting depth images as input and uses a ZF-Net \cite{zfnet} inspired convolutional neural network to generate bounding boxes for a detected hand in the input image. The image is then cropped to the dimensions of this bounding box and passed to the skeleton-difference loss neural network. This network seeks to minimize the angle between all the joints of the hand skeleton and between the joint length and ground truth joint length. This network predicts the location of the hand joints using a one-hundred-and-one layer recurrent neural network. Additionally, this implementation is developed to be robust to occlusions of the hand by objects held in the hand.\\

Hand pose estimation itself is a sub-field of full-body pose estimation which is a problem solved by Toshev \cite{deep_pose} and which takes advantage of a hierarchical progression of increasingly-fine-grained pose regressors for the joints of a whole body and is comprised at its core of simple convolutional layers stacked after one another. \\

It is evident that the advent of deep learning has yielded a large number of new approaches to hand pose estimation and that the extensive use of convolutional neural networks is the modern approach most preferred in academia. This is due to the ease of not having to implement detailed representations of low-level hand shapes, patterns and methods of identifying these features in input imagery but rather instead training a deep learning system to identify and learn these low-level abstractions using vast amounts of training data and optimized architectures such as the convolutional neural network. \\

The majority of hand pose estimation systems surveyed above rely on some form of detection of the user's hand or palm as a provisional step to hand pose estimation. This is because detecting the hand allows a system to crop the input RGB or depth image to only those dimensions that include a hand and allow background noise and interference to be suppressed - increasing the accuracy of hand pose estimation and hand joint localisation systems as well as decreasing the computational complexity and training time needed to train these large neural network systems to be robust against noise and various different inputs. The classical approaches to hand detection and fingertip detection mainly rely on skin color or hue segmentation followed by contour and curvature-based algorithms that can detect the contours and curves of the user's hand and locate these in a segmented input image - however these have their shortcomings when it comes to noise tolerance and reliability. \\

Returning to the implementation of augmented reality systems - many augmented reality applications have been created that rely on hand and gesture input and all use some form of global or world coordinate system shared by real-world and virtual objects. This allows the position of these objects to be related to each other and for collisions and interactions between these objects to be modelled and rendered. Classically, paper fiducial markers were used to orient the camera and synchronise virtual as well as real-world objects to the global coordinate system, however modern approaches have dispensed with these as depth-tracking devices such as the Microsoft Kinect and Leap Motion sensors have become available for academic use and can fulfill much the same purpose. \\

Considering the broad body of literature on hand gesture control of virtual objects and their applicability to augmented reality applications, several design choices have been informed for the system to be implemented. It is evident that the preferred approach in the literature for hand pose estimation and gesture recognition is to use a deep-learning architecture to regress hand joint coordinates from either a depth or standard RGB camera input. Gesture recognition can either be performed by deep-learning approaches or by classical calculations depending on the complexity of the required gestures.\\

Augmented reality and the combination of virtual reality objects with real-world objects can create immersive and useful applications when a suitable input camera is used and a shared coordinate system is established to track both virtual and real objects and prevent collisions between them. The use of a RANSAC-based algorithm on normal vectors calculated from the depth information of a suitable camera can be used to estimate planar surfaces and place objects on those surfaces in augmented reality or even to aid in collision avoidance between objects in the shared coordinate space. \\

The use of large-scale neural networks in the literature for gesture recognition and joint regression is mainly used for detecting large numbers of gestures or finding the coordinates of tens of hand joints in an input image. Scaling down the output of these networks can significantly reduce the complexity needed in their construction and in the amount of layers needed for accurate operation. Thus, designing a system that only differentiates between a handful of gestures or localizes one or two hand landmarks will be much cheaper to develop computationally and allow better operation using an embedded device and first-principles algorithms. \\

The additional use of pre-processing using skin color and hue segmentation, depth data as well as hand detection and bounding-box algorithms - whether classical or using a deep-learning approach, will also greatly reduce the complexity of the gesture recognition and hand pose estimation design work to be completed. In conclusion, a system will be developed that can accept user gesture input using a deep-learning approach coupled with targeted pre-processing and then translate that gesture into meaningful instructions for a virtual object present in an augmented reality scene that presents realistic interactions between it and real-world objects and uses a global coordinate system to prevent object collisions. \\

\subsection{Final implementation progress}

Five weeks remain until the final project report is due and so focus will now shift from the system's implementation to completing that report. Work has already begun on it in the form of scaffolding out the section subheadings for the design and implementation section and in the form of the literature review progress above. The surface detection and object detection systems still need to be integrated and the embedded platform implementation actually working to a demonstration-worthy point. Additionally, the neural networks used for hand detection need to be improved and so work will still continue in parallel with the report writing.